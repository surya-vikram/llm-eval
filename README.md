# llm-eval
A framework to compare the outputs of multiple LLMs on Boolean question-answering tasks.
